{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25900946-2351-4d40-a84f-2a9ebf320579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Archiv\\Studium\\Master\\6.-Semester\\Masters_Thesis\\Git\\acoustic_covid_detection\\python\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from jupyter_utils import jupyter_setup, load_tracker\n",
    "import ipywidgets as widgets\n",
    "jupyter_setup()\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as ex\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d955e93-e774-45a3-92fd-06395b1e4635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tracker from: D:/Archiv/Studium/Master/6.-Semester/Masters_Thesis/Git/acoustic_covid_detection/python/run/tracker_saves/2023-02-20_brogrammers_brogrammers_new_feature_set_class_train_val_test_split.pickle\n",
      "\n",
      "Parameters used for finding the best performing epoch:\n",
      "           metric_used:                   auc_roc\n",
      "           smoothing:                     10\n",
      "           ignore_first_n_epochs:         10\n",
      "\n",
      "Datasets used:\n",
      "brogrammers_new\n",
      "           dataset_class:                 <class 'datasets.BrogrammersMFCCDataset'>\n",
      "           participants_file:             2023_02_20_brogrammers_settings_new.pickle\n",
      "           augmented_files:               ['2023_02_20_brogrammers_settings_new_augmented.pickle']\n",
      "           train_set_label_counts:        label '0': 2134  -  label '1': 1305\n",
      "           types_of_recording:            cough-heavy\n",
      "\n",
      "Audio Processing Parameters:\n",
      "           type_of_features:              mfcc\n",
      "           n_time_steps:                  259\n",
      "           n_features:                    15\n",
      "           sample_rate:                   22050\n",
      "           n_fft:                         2048\n",
      "           window_length:                 2048\n",
      "           hop_size:                      512\n",
      "           fmin:                          0\n",
      "           fmax:                          11025\n",
      "           hop_size_ms:                   23.22\n",
      "           window_length_ms:              92.88\n",
      "           duration_seconds:              6.01\n",
      "           fft_res_hz:                    10.77\n",
      "\n",
      "Predetermined Time Domain Augmentations:\n",
      "           AddGaussianNoise:              'probability': 0.8, 'min_amplitude': 0.0003, 'max_amplitude': 0.02\n",
      "           PitchShift:                    'probability': 0.8, 'min_semitones': -3, 'max_semitones': 3\n",
      "           TimeStretch:                   'probability': 0.8, 'min_rate': 0.85, 'max_rate': 1.15\n",
      "           Gain:                          'probability': 0.8, 'min_gain_in_db': -40, 'max_gain_in_db': 16\n",
      "           model:                         BrogrammersModel\n",
      "           optimizer:                     Adam \n",
      "           loss_function:                 BCEWithLogitsLoss\n",
      "\n",
      "Tracker contains:\n",
      "           parameter runs:                6\n",
      "           folds per run:                 5\n",
      "           epochs:                        100\n"
     ]
    }
   ],
   "source": [
    "tracker = load_tracker()\n",
    "tracker.summarize(detail=\"compact\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5277788f-7ece-4a1e-8160-555c08ad3693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c3c9b9b26847af979d517c700410c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='dataset_and_model'), Checkbox(value=True, description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = widgets.interact(tracker.get_data_and_model_params, dataset_and_model=False, audio_processing=True, augmentations=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce061a2-2a7c-4d7f-9a51-db8d8457e651",
   "metadata": {},
   "source": [
    "# Compute Tracker Metrics:\n",
    "when loading the tracker (load_tracker()), the recorded data will be loaded and preprocessed (cross validation mean/std and much more will be calculated). This is essential. Otherwise none of the following functionality works. You can call the preprocessing function manually (tracker.compute_overall_metrics()) to change the settings for processing. The available parameters are: <br>\n",
    "- smooth_n_samples=10 <br>\n",
    "This sets the number of samples over which a (weighted) moving average filter is applied before looking for a min/max of the curve or calculate the mean.\n",
    "- ignore_first_n_epochs=5 <br>\n",
    "Simply do not take the first n epochs into account when looking for a min/max\n",
    "- metric_used_for_performance_analysis=\"auc_roc\" <br>\n",
    "The epoch on which the performance for all metrics will be evaluated for a specific run will be chosen by looking for the min/max in this metric. This means, that if the AUC ROC curve is chosen in the argument, it will look for the argmax of the AUC ROC curve and evaluate loss, accuracy, F1 etc in this exact epoch even when they have their extremum in another epoch  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f05b3fcd-7bd6-41c7-9027-e462fe98e811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters used for finding the best performing epoch:\n",
      "           metric_used:                   auc_roc\n",
      "           smoothing:                     10\n",
      "           ignore_first_n_epochs:         10\n"
     ]
    }
   ],
   "source": [
    "tracker.compute_overall_metrics(smooth_n_samples=10, ignore_first_n_epochs=10, metric_used_for_performance_analysis=\"auc_roc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8552cbf1-2f4d-41f3-8783-d8a808f14e34",
   "metadata": {},
   "source": [
    "# Evaluate the Cross Validation Results\n",
    "For each run within a cross validation setting, the epoch of the min/max will be calculated and all metrics evaluated at this metric (after being smoothed as specified). The mean and std are calculated from the (in the default case) epoch where AUC ROC hits the global maximum. The resulting table can be adjusted with the jupyter widgets: <br>\n",
    "- The styled table can be sorted by metric values (first drop down)  <br>\n",
    "- by ticking \"show_full_df\" you can also display e.g. the standard deviation of each metric\n",
    "- If you tick \"highlight parameters\" all hyperparameters, that changed thoughout the different runs, are highlighted by value. <br>\n",
    "- The sliders below can be adjusted to filter out all runs with results worse than the slider setting  <br>\n",
    "For more details (standard deviation e.g.), the cell below does exaclty the same but with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d542739c-2cb0-4fef-a26c-0e4926bed5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0230f89e0641b98d83df687b64bae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='sort_descending_by_metric', options=('auc_roc', 'loss', 'accuracy'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sortby(sort_descending_by_metric, highlight_hyperparameters, show_full_df, **kwargs):\n",
    "    # thresholds = {\"auc_roc\": auc_threshold}\n",
    "    if show_full_df:\n",
    "        df = tracker.full_metric_performance_df\n",
    "    else:\n",
    "        df = tracker.compact_metric_performance_df\n",
    "    style = tracker.style_metric_performance_df(df, sort_by=sort_descending_by_metric, highlight_hyperparameters=highlight_hyperparameters, filter_thresholds=kwargs)\n",
    "    return style\n",
    "_ = widgets.interact(sortby, sort_descending_by_metric=tracker.metrics_used, highlight_hyperparameters=True, show_full_df=False,\n",
    "                auc_roc=widgets.FloatSlider(min=0.0, max=1.0, value=0.7, step=0.005),\n",
    "                accuracy=widgets.FloatSlider(min=0.0, max=1.0, value=0.7, step=0.005),\n",
    "                tpr=widgets.FloatSlider(min=0.0, max=1.0, value=0.4, step=0.005),\n",
    "                tnr=widgets.FloatSlider(min=0.0, max=1.0, value=0.8, step=0.005)\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2166ddb-c2e4-4485-a148-ed5344e27dfd",
   "metadata": {},
   "source": [
    "# Box Plot the Hyperpaprameter Performances\n",
    "Each metric is viewed separately. You can also choose to only look at the n_best performing hyperparameter runs. <br>\n",
    "Depending on whether \"sort_by_current_metric\" is ticked this means something different. If it is not ticked, all runs will be sorted by the metric defined in the beginning (tracker.compute_overall_metrics()). Otherwise it will be sorted by the performance of the metric you have chosen to look at. This is very usefull  to get insight on which setting of a specific hyperparameter affects specific metrics. Some hyperparameter may for example improve the AUC ROC a bit but in the F1 or tnr metric they clearly hava a negative impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f253649-2a96-486f-8eca-61058c080721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418113a36f1343989f2983c95e0e4bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='metric', options=('auc_roc', 'loss', 'accuracy', 'f1_score', 'auc_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hyperparameters = (list(tracker.crossval_runs[0].parameters.keys()))\n",
    "_ = widgets.interact(tracker.boxplot_run_statistics, metric=tracker.metrics_used,\n",
    "                     show_n_best_runs=widgets.IntSlider(value=tracker.n_hyperparameter_runs, min=1, max=tracker.n_hyperparameter_runs),\n",
    "                     color_by_hyperparameter=[None, *tracker.hyperparameters],\n",
    "                     sort_by_current_metric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaa3bef-ca38-4116-a256-08ce66a06edd",
   "metadata": {},
   "source": [
    "# Metric Performance over Epochs\n",
    "Here we can analyze the cross validation mean of each hyperparameter run over the epochs. You can switch between train and eval mode and choose a specific metric to look at. The solid line is the mean over all folds within a crossval run. This is calculated epoch-wise, so at each epoch the smoothed metric performance of all folds is averaged. In the same way, the standard devaition is calculated and shown as area plot. <br>\n",
    "You can choose to display the area plot (std) and also to view the fold within each crossval run + the epoch in which the in/max of the specified metric (in tracker.compute_overall_metrics()) was found. Additionally you can choose to view only the n best performing hyperparameter runs. The ranking is calculated by the metric set at the beginning (tracker.compute_overall_metrics()). <br>\n",
    "You can change the number of samples to smooth the curves with. The default setting was also set in the compute_overall_metrics method. This is only for visualization and does not change the settings which are used to calculate the final crossval performance.<br>\n",
    "Below is a similar method but it only shows 1 crossval run which can be switched with the dropdown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3407a633-e593-4d24-86b0-afb12c88baf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9ec0ebdc904ab7ac129da3949370ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='mode', options=('eval', 'train'), value='eval'), Dropdown(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = widgets.interact(tracker.show_all_runs, \n",
    "                     mode=[\"eval\", \"train\"], \n",
    "                     metric=tracker.metrics_used, \n",
    "                     n_samples_for_smoothing=widgets.IntSlider(value=tracker.performance_eval_params[\"smoothing\"], min=1, max=15), \n",
    "                     show_separate_folds=False, \n",
    "                     show_std_area_plot=False,\n",
    "                     show_n_best_runs=widgets.IntSlider(value=5, min=1, max=tracker.n_hyperparameter_runs),\n",
    "                     color_by_hyperparameter=[None, *tracker.hyperparameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab612294-eca8-489d-8743-794ae7278edf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_list_of_best_n_runs(sort_by_metric=True, metric=\"auc_roc\"):\n",
    "    # this way, the dropdown menu of run_id below is sorted by metric performance specified\n",
    "    if sort_by_metric:\n",
    "        hyperparameter_settings = [str(tracker.crossval_runs[idx].parameters) for idx in tracker.get_indices_of_best_n_runs(tracker.n_hyperparameter_runs, metric)]\n",
    "    else:\n",
    "        hyperparameter_settings = tracker.run_ids\n",
    "    return hyperparameter_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9948dab-cc00-4282-9254-04f0a52da1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36c09dbd1b6429188d29ae116309507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='run_id', options=(\"{'batch': '32', 'lr': '5e-05', 'wd': '0.0001', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = widgets.interact(tracker.show_single_run, \n",
    "                 run_id=get_list_of_best_n_runs(sort_by_metric=True, metric=\"auc_roc\"),\n",
    "                 mode=[\"eval\", \"train\"], \n",
    "                 metric=tracker.metrics_used, \n",
    "                 n_samples_for_smoothing=widgets.IntSlider(value=tracker.performance_eval_params[\"smoothing\"], min=1, max=15), \n",
    "                 show_separate_folds=True, \n",
    "                 show_std_area_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f818736-cefe-4d84-b8d5-36f28f5e11b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e205f6-8342-4672-8511-5a99c6039102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python_v3-8)",
   "language": "python",
   "name": "python_v3-8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
