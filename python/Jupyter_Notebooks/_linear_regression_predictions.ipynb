{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "481d4a51-353d-4f68-92c4-4312083795c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Michi\\acoustic_covid_detection\\python\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from jupyter_utils import jupyter_setup, load_tracker\n",
    "jupyter_setup()\n",
    "import os\n",
    "from evaluation_and_tracking import IDPerformanceTracker\n",
    "import torch\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, precision_recall_curve\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e9e3099-d7ef-41f5-b23f-072ac22a9997",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_PERFORMANCE_TRACKING = \"test_linearRegression.pickle\"\n",
    "id_performance = IDPerformanceTracker(ID_PERFORMANCE_TRACKING)\n",
    "id_performance.df = id_performance.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7617b0a2-5c8c-475c-9037-4a64137854b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2898"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cc85291-d302-41b2-96e8-69d045d319a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = id_performance.df[id_performance.df.set_type == \"eval\"]\n",
    "test_data = id_performance.df[id_performance.df.set_type == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "657d7893-83e8-49fd-b626-b114a41f2128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 466)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ids = eval_data.ID.unique()\n",
    "test_ids = test_data.ID.unique()\n",
    "len(eval_ids), len(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd776f7b-1d01-4ce6-b88a-db4d2e593e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['combined_speech', 'combined_coughs', 'combined_vowels',\n",
       "       'combined_breaths'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recording_types = eval_data.rec_type.unique()\n",
    "recording_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7488156e-8c24-4a36-8766-b886dea28553",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Participant:\n",
    "    def __init__(self, identifier, df, allow_n_missing_recordings=3):\n",
    "        self.id = identifier\n",
    "        self.cough = self.get_single_prediction(\"combined_coughs\", df)\n",
    "        self.speech = self.get_single_prediction(\"combined_speech\", df)\n",
    "        self.breath = self.get_single_prediction(\"combined_breaths\", df)\n",
    "        self.vowels = self.get_single_prediction(\"combined_vowels\", df)\n",
    "        self.label = df[df.ID == identifier].label.values[0]\n",
    "        \n",
    "        no_recording = 0\n",
    "        if self.cough is None:\n",
    "            no_recording += 1\n",
    "            self.cough = 0\n",
    "        if self.speech is None:\n",
    "            no_recording += 1\n",
    "            self.speech = 0\n",
    "        if self.breath is None:\n",
    "            no_recording += 1\n",
    "            self.breath = 0\n",
    "        if self.vowels is None:\n",
    "            no_recording += 1\n",
    "            self.vowels = 0\n",
    "        if no_recording > allow_n_missing_recordings:\n",
    "            raise ValueError(\"there is at least 1 recording not present\")\n",
    "            \n",
    "            \n",
    "        \n",
    "    def get_single_prediction(self, rec_type, df):\n",
    "        idx = np.logical_and(df.ID == self.id, df.rec_type == rec_type)\n",
    "        n_entries = len(df[idx])\n",
    "        if n_entries == 1:\n",
    "            prediction = df[idx].prediction.values[0][-1]\n",
    "        elif n_entries == 0:\n",
    "            # print(\"error\")\n",
    "            # raise ValueError(\"No Entry for this\")\n",
    "            # prediction = 0\n",
    "            prediction = None\n",
    "        else:\n",
    "            raise ValueError(\"there cannot be more than one entry with the same ID and rec type\")\n",
    "        # add sigmoid???\n",
    "        # print(prediction)\n",
    "        return prediction\n",
    "    \n",
    "    def get_all_predictions(self):\n",
    "        return np.array([self.cough, self.speech, self.breath, self.vowels])\n",
    "    # def calculate AUCROC, accuracy, loss for one category and after linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4eadf7c4-9dab-437a-8f0c-7029f0d112c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data[eval_data.ID == \"00xKcQMmcAhX8CODgBBLOe7Dm0T2\"].label.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5965a12f-ebe9-44cd-9eef-75a99599b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant = Participant(eval_ids[0], eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed088962-e254-4ab9-beb3-d235af58c2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.63678253, -0.19896442, -0.26176679, -0.22631815,  1.        ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(participant.get_all_predictions(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8305e6f-5d69-4b2f-bb7e-ebc2a1e2fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linregr_matrices(eval_ids, data):\n",
    "    predictions_matrix = np.array([])\n",
    "    for i, participant_id in enumerate(eval_ids):\n",
    "        try:\n",
    "            participant = Participant(participant_id, data)\n",
    "        except ValueError:\n",
    "            # print(\"error\")\n",
    "            continue\n",
    "        # print(participant_id)\n",
    "        if i == 0 or len(predictions_matrix) == 0:\n",
    "            predictions_matrix = participant.get_all_predictions()\n",
    "            labels = np.array([participant.label])\n",
    "        else:\n",
    "            predictions = participant.get_all_predictions()\n",
    "            # print(predictions)\n",
    "            predictions_matrix = np.vstack([predictions_matrix, predictions])\n",
    "            labels = np.append(labels, participant.label)\n",
    "        # print(participant_id)\n",
    "    return predictions_matrix, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d9493a6-33f4-41d5-8d43-df3a4049f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(labels, predictions, threshold=0.5, verbose=False):\n",
    "    labels_bool = labels > threshold\n",
    "    # predictions = torch.sigmoid(torch.Tensor(predictions))\n",
    "    predicted_labels = predictions > threshold\n",
    "    n_correctly_predicted = np.sum(predicted_labels == labels_bool) / len(predictions)\n",
    "    return np.round(n_correctly_predicted*100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8456d629-29b0-4a66-90d8-c96ad7d603c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aucroc(labels, predictions):\n",
    "    # using mixup, the resulting labels are no longer binary but continous between 0 and 1\n",
    "    # we round to get any kind of result but for the training data, the auc-roc is not quite meaningful\n",
    "    # labels = np.round(self.labels)\n",
    "    # try:\n",
    "    fpr, tpr, thresh = roc_curve(labels, predictions)\n",
    "    aucroc = auc(fpr, tpr)\n",
    "    # except ValueError:\n",
    "    #     # fpr, tpr, thresh = 0, 0, 0\n",
    "    #     aucroc = 0.0\n",
    "    return np.round(aucroc*100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d0f33e84-4a0a-45b8-b02b-3a1997d13fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix_parameters(labels, predictions, threshold=0.5, verbose=False):\n",
    "    predictions_bool = predictions > threshold\n",
    "    labels_bool = labels > threshold\n",
    "    confusion_mat = confusion_matrix(labels_bool, predictions_bool)\n",
    "    mat = np.flip(confusion_mat)\n",
    "    mat = np.concatenate([mat, np.expand_dims(mat.sum(axis=1), 1)], axis=1)\n",
    "    mat = np.concatenate([mat, np.expand_dims(mat.sum(axis=0), 0)], axis=0)\n",
    "    if verbose:\n",
    "        print(\"##########################################################################\\n\")\n",
    "        print(pd.DataFrame(mat, columns=[\"Pred. [+]\", \"Pred. [-]\", \"True Total\"],\n",
    "                           index=[\"True [+]\", \"True [-]\", \"Pred. Total\"]))\n",
    "\n",
    "    # returns parameters in the following order: tn, fp, fn, tp\n",
    "    return confusion_mat.ravel()\n",
    "\n",
    "def get_rates_from_confusion_matrix(confusion_mat, verbose=False):\n",
    "    TN, FP, FN, TP = confusion_mat\n",
    "    total_negatives = TN + FP\n",
    "    total_positives = FN + TP\n",
    "    tpr = round(TP / total_positives, 4)  # also known as recall\n",
    "    tnr = round(TN / total_negatives, 4)\n",
    "    fnr = round(FN / total_positives, 4)\n",
    "    fpr = round(FP / total_negatives, 4)\n",
    "    precision = round(TP / (TP + FP), 4)\n",
    "    if verbose:\n",
    "        print(pd.DataFrame(dict(tpr=tpr * 100, fpr=fpr * 100, tnr=tnr * 100, fnr=fnr * 100), index=[0]))\n",
    "    return tpr, fpr, tnr, fnr, precision\n",
    "\n",
    "def get_auc_prec_recall(labels, predictions):\n",
    "    precision, recall, _ = precision_recall_curve(labels, predictions)\n",
    "    auc_preision_recall = auc(recall, precision)\n",
    "    return auc_preision_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a96a944-3a1f-4ea4-83a5-04af4cdde833",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49238472-05d3-4c68-996c-f14a0017fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(A):\n",
    "    return torch.sigmoid(torch.Tensor(A)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce18eb1f-f4b5-478b-9437-b72c14cf3345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_linregr_matrx(A):\n",
    "    # include further components for the linear regression, like a constant, square of each component, inverse squre, square root,...)\n",
    "    # bias = np.ones((A.shape[0], 1))\n",
    "    # sign = np.sign(A)\n",
    "    absolute = np.abs(A)\n",
    "    squares = np.power(A, 2)\n",
    "    # cubes = np.power(A, 3)\n",
    "    roots = np.power(absolute, 1/2)\n",
    "    # cuberoots = np.power(absolute, 1/3)\n",
    "    # power_four = np.power(A, 4)\n",
    "    # power_five = np.power(A, 5)\n",
    "    return np.concatenate((A, squares, roots), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617c7aa1-fe45-4526-9fa4-b719a16e4c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fac24417-76c1-48f2-bf68-8af55274a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_val, y_val = get_linregr_matrices(eval_ids, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ddb05cb-7813-46ea-9107-5559e1e84919",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_test, y_test = get_linregr_matrices(test_ids, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aac753ee-0746-4c29-902b-ecccd0165738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 466)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(A_val), len(A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c54d78e3-1877-4636-ae6b-5236e70b7048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_val = sigmoid(A_val)\n",
    "# A_test = sigmoid(A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e29c90b-1aa8-4f0c-b134-1d2191e8cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_val = extend_linregr_matrx(A_val)\n",
    "A_test = extend_linregr_matrx(A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "783e46c6-55c3-4adb-95e3-7d2db4e5c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(A_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05d2528e-edd4-4e66-95c7-2dcb9478b240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90.2, 84.2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_pred = model.predict(A_val)\n",
    "get_aucroc(y_val, y_val_pred), get_accuracy(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be371ddf-2593-4510-9b50-109a1c82c8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89.4, 92.5)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = model.predict(A_test)\n",
    "get_aucroc(y_test, y_test_pred), get_accuracy(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ed785fd-8143-481c-a71c-5b9fb8b438ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2152338 ,  0.26773116,  0.37782496,  0.07782764,  0.26772716,\n",
       "        0.2044399 ,  0.35342703,  0.01884383, -0.12775933, -0.13247216,\n",
       "       -0.04418645, -0.0956226 ])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2704e834-e0e5-43cf-a5d9-041046219fe5",
   "metadata": {},
   "source": [
    "# confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "56274d4d-46d6-41b2-8041-50b44998ecff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################################################\n",
      "\n",
      "             Pred. [+]  Pred. [-]  True Total\n",
      "True [+]            70         40         110\n",
      "True [-]            17        233         250\n",
      "Pred. Total         87        273         360\n",
      "     tpr  fpr   tnr    fnr\n",
      "0  63.64  6.8  93.2  36.36\n",
      "\n",
      "precision 80.5\n",
      "\n",
      "F1: 71.1\n",
      "AUC pr-recall: 82.0\n"
     ]
    }
   ],
   "source": [
    "mat = get_confusion_matrix_parameters(y_val, y_val_pred, verbose=True)\n",
    "tpr, _, tnr, _, precision = get_rates_from_confusion_matrix(mat, verbose=True)\n",
    "recall = tpr\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"\\nprecision {round(precision*100, 1)}\")\n",
    "print(f\"\\nF1: {round(f1_score*100, 1)}\")\n",
    "print(f\"AUC pr-recall: {round(get_auc_prec_recall(y_val, y_val_pred)*100, 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "579016c7-4993-4590-ae87-d142d0c3560e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################################################\n",
      "\n",
      "             Pred. [+]  Pred. [-]  True Total\n",
      "True [+]            29         31          60\n",
      "True [-]             4        402         406\n",
      "Pred. Total         33        433         466\n",
      "     tpr   fpr    tnr    fnr\n",
      "0  48.33  0.99  99.01  51.67\n",
      "\n",
      "precision 87.9\n",
      "F1: 62.4\n",
      "AUC pr-recall: 70.4\n"
     ]
    }
   ],
   "source": [
    "mat = get_confusion_matrix_parameters(y_test, y_test_pred, verbose=True)\n",
    "tpr, _, tnr, _, precision = get_rates_from_confusion_matrix(mat, verbose=True)\n",
    "recall = tpr\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"\\nprecision {round(precision*100, 1)}\")\n",
    "print(f\"F1: {round(f1_score*100, 1)}\")\n",
    "print(f\"AUC pr-recall: {round(get_auc_prec_recall(y_test, y_test_pred)*100, 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "92b9a989-cd3e-478e-8c0f-3527cea6dc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4833"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0cbff69c-df9b-4a91-a8cd-4e43d742eb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8787878787878788"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "29/(33) #precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "93bfa406-d16f-43f6-85f4-c4221898111b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48333333333333334"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "29/(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "id": "9b99491a-ebb0-4bcc-bf95-dcaadb3f6c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval set:    77.4 83.8\n",
      "test set:    79.2 87.0\n",
      "#################################################\n",
      "eval set:    83.9 87.8\n",
      "test set:    78.8 88.9\n",
      "#################################################\n",
      "eval set:    82.6 82.7\n",
      "test set:    82.1 86.0\n",
      "#################################################\n",
      "eval set:    76.2 82.7\n",
      "test set:    76.7 87.6\n",
      "#################################################\n"
     ]
    }
   ],
   "source": [
    "# AUCROC and accuracy for validation set and test set before linear regression\n",
    "# keep in mind that IDs that were excluded for various reasons (e.g. audio quality) were set to have a prediction right in the middle (0.5) which decreases the performance, especially accuracy\n",
    "for rec_type_idx in range(4): \n",
    "    pred = torch.sigmoid(torch.Tensor(A_val))[:, rec_type_idx].numpy()\n",
    "    print(\"eval set:   \", get_aucroc(y_val, pred), get_accuracy(y_val, pred))\n",
    "    pred = torch.sigmoid(torch.Tensor(A_test))[:, rec_type_idx].numpy()\n",
    "    print(\"test set:   \", get_aucroc(y_test, pred), get_accuracy(y_test, pred))\n",
    "    print(\"#################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e62f2-26bb-40ff-a5af-3cf1bf1e4511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
